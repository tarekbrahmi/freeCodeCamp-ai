{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba7efbb",
   "metadata": {},
   "source": [
    "For this challenge, you will complete the code below to classify images of dogs and cats. You will use Tensorflow 2.0 and Keras to create a convolutional neural network that correctly classifies images of cats and dogs at least 63% of the time. (Extra credit if you get it to 70% accuracy!)\n",
    "\n",
    "Some of the code is given to you but some code you must fill in to complete this challenge. Read the instruction in each text cell so you will know what you have to do in each code cell.\n",
    "\n",
    "The first code cell imports the required libraries. The second code cell downloads the data and sets key variables. The third cell is the first place you will write your own code.\n",
    "\n",
    "The structure of the dataset files that are downloaded looks like this (You will notice that the test directory has no subdirectories and the images are not labeled):\n",
    "```\n",
    "cats_and_dogs\n",
    "|__ train:\n",
    "    |______ cats: [cat.0.jpg, cat.1.jpg ...]\n",
    "    |______ dogs: [dog.0.jpg, dog.1.jpg ...]\n",
    "|__ validation:\n",
    "    |______ cats: [cat.2000.jpg, cat.2001.jpg ...]\n",
    "    |______ dogs: [dog.2000.jpg, dog.2001.jpg ...]\n",
    "|__ test: [1.jpg, 2.jpg ...]\n",
    "```\n",
    "\n",
    "You can tweak epochs and batch size if you like, but it is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "075f93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd100ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project files\n",
    "PATH = 'cats_and_dogs'\n",
    "\n",
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')\n",
    "test_dir = os.path.join(PATH, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bcd35f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of files in each directory. The train and validation directories\n",
    "# each have the subdirecories \"dogs\" and \"cats\".\n",
    "total_train = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "total_val = sum([len(files) for r, d, files in os.walk(validation_dir)])\n",
    "total_test = len(os.listdir(test_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc04ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 51 1001\n"
     ]
    }
   ],
   "source": [
    "print(total_train,total_test,total_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6ee8e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for pre-processing and training.\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4140b51",
   "metadata": {},
   "source": [
    "Now it is your turn! Set each of the variables below correctly. (They should no longer equal `None`.)\n",
    "\n",
    "Create image generators for each of the three image data sets (train, validation, test). Use `ImageDataGenerator` to read / decode the images and convert them into floating point tensors. Use the `rescale` argument (and no other arguments for now) to rescale the tensors from values between 0 and 255 to values between 0 and 1.\n",
    "\n",
    "For the `*_data_gen` variables, use the `flow_from_directory` method. Pass in the batch size, directory, target size (`(IMG_HEIGHT, IMG_WIDTH)`), class mode, and anything else required. `test_data_gen` will be the trickiest one. For `test_data_gen`, make sure to pass in `shuffle=False` to the `flow_from_directory` method. This will make sure the final predictions stay is in the order that our test expects. For `test_data_gen` it will also be helpful to observe the directory structure.\n",
    "\n",
    "\n",
    "After you run the code, the output should look like this:\n",
    "```\n",
    "Found 2000 images belonging to 2 classes.\n",
    "Found 1000 images belonging to 2 classes.\n",
    "Found 50 images belonging to 1 classes.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9a42aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(train_dir,rescale=True)\n",
    "validation_image_generator = ImageDataGenerator(validation_dir,rescale=True)\n",
    "test_image_generator = ImageDataGenerator(test_dir,rescale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c751eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(train_dir,(IMG_HEIGHT,IMG_WIDTH),batch_size=batch_size)\n",
    "val_data_gen = validation_image_generator.flow_from_directory(validation_dir,(IMG_HEIGHT,IMG_WIDTH),batch_size=batch_size)\n",
    "test_data_gen = test_image_generator.flow_from_directory(test_dir,(IMG_HEIGHT,IMG_WIDTH),batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc0bf70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x7f3d52cabf40>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae30ff1",
   "metadata": {},
   "source": [
    "The `plotImages` function will be used a few times to plot images. It takes an array of images and a probabilities list, although the probabilities list is optional. This code is given to you. If you created the `train_data_gen` variable correctly, then running the cell below will plot five random training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc9de7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr, probabilities=False):\n",
    "    fig, axes = plt.subplots(\n",
    "        len(images_arr), 1, figsize=(5, len(images_arr) * 3))\n",
    "    if probabilities is False:\n",
    "        for img, ax in zip(images_arr, axes):\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "    else:\n",
    "        for img, probability, ax in zip(images_arr, probabilities, axes):\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            if probability > 0.5:\n",
    "                ax.set_title(\"%.2f\" % (probability*100) + \"% dog\")\n",
    "            else:\n",
    "                ax.set_title(\"%.2f\" % ((1-probability)*100) + \"% cat\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4e6d18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJoAAAM9CAYAAABkDmrcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYsklEQVR4nO3de5BkdX2G8WfYRVcQNBDFGyhiYQyoEMWUiiblJWoZBRWTiGggVqIG72JiWSoaU2g0JN5iUMFoVAwKJirRELxEUVSUQi4GjYrAsqzicl9gd2G388d32pnt7Z7u6Tn99nTP86kCZnp6ug8zz5zr75wz02q1kEZtp3FPgFYGQ1OEoSnC0BRhaIpY3efrbpJqsWa6PegcTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSlidZ+vz0SmQlPPOZoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqaIftfeaEWmQtOk6/VanKMpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0Razu8/WZyFRo6jlHU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFNHv2hutyFQo7DbqV7/zKF686/VanKONxY3ALWN8/9cAn4u+o6GNxcnAWeOeiKh+i06NxGvHPQFxztEUYWiKMDR1sQX4x0Zf0dDUxQyw69ynlwKfWOIrtloL7ipzP9pUOg+4F7DPYE+/GrgceMxAz3Y/2uT6IXB8g6/3KAaODOA+9I/ssMPgjjt6ftnQJsINwIXjnoiFveWrsKr3AnAKQ3sY0PsvazI9Ajipwdd7E/ClLo+3gAcP95IHXwQzvXfLTmFoXwNWhd7rGODcwPusodapmrKBHQ6B7QdsngG+NeRr7stCt6VwY2BJNlMHV5oIe1/gl9Rx0Baw9+znbV+g5kIfaOC9tlDzmNk50P7A16mWe7Vy6qlw8cXw9rf3e/GurzDC0LYBuwM3MZUzzsZtnf1v+2e1je0Dbs3+08TP8ljgCcA/AefA1stgp70Xvk9Oq1X/7NT3/dOhAWwCHkhtH2v5aFE9tH+9jd6JaRyhaQVyP9ritIArxz0RQ7hi3BPQlaH1tAV44bgnYgjPYeEF0Xpqq7NpFy34vgOGthn4TiOTMznuDPxPn+dcDFw/+klZlO+z8DrXWcC3R/C+b2Bug2ZHA4Z2C/DpRiZnupxNzSEmydHAM0bwumey0DhaNwbUNDcGluYynKsPz9B6up0a238t8LfAz4EzxjpFk8zQetoJ+H3gZuB04KHUaWoahutoPW2hdhV8BrgEeOR4J2dyuI62ONu4gXN5CmswsqVzjtZTi61czTW0uDcvYqWd8LsETRzr/A7wPuCTTU3UBNgGbKRGomgATYS2jdr7O5KLg2g6dA1twEsitJgbH+VqnRZvwGrWU2fOSMMZcNG5lRpivMfop0iTbim7NzYARzY4LVpp3L2hprnDVuNjaIowNEUMGNqNwIkjnRBNtwFD2wicMtIJ0XQbMLQ9aeZUfK1U7t5Q09y9ofExNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEWs7vP1mchUaOo5R1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIrod5GXVmQqNE26XhjIOZoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIgxNEYamiNV9vj4TmQpNPedoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMTRGGpghDU4ShKcLQFGFoijA0RRiaIvpde6MVmQpNk67Xa3GOpghDU4ShKcLQFGFoijA0RRiaIgxNEYamCENThKEpwtAUYWiKMDSeCVww7omYejOt1oIjgVbAMKFt1MgW70bUkK4/yH7j0VYAZ+oJ/pQVYWiKMDRFGJoiDE0RhqYIQ1OEoSnC0BRhaIowNEUYmiIMbSgnAB8Z8nvfB7ynwWmZDA4T+rX2/+rM7McLDRu6Y/brq4Z4j62zH6/ueM+p4dWEFvZbwC+piO7Mwn9jq1l8ZACfBN7A3Oisw4HvDvE6k8c52nbuD2yg/v5uYsrmNCldf2grPLSbgDXAnUb4Hltn3+c3Rvgey4qLzh29heEXXdcAGwd43lXAi4Z8j+mxwudoS/EO4BDgieOekCX4EbAPsEuTL+ocbWHfALYs4vmvZ8fIvkyd7DIJLgDeRW0Ajd4KD+0c4OrZjz8HfJza6hzW6cztvljOvkttAb8O2DfyjisstJ8CZ837/Erg5tmPT5z9fClzpJOAnfs853vAeUt4jyZcBbyQ2qWTscLW0f4DOBM4eUSvfzzwJhY+i/E86sf6uyOahrFbqeto11M7SQF+Bzhm9uNTgW83/F4Ppv++t0cxxZH1tAJCuwvwB7Mf7wM8dvbjh89+Pt+RwOYlvNeRwBFM3YKgAVMQ2nHAo4Erenx9DfD7HY99DLgEuG/H469h6Sfvf5FsaEcD64CnAZuC77s4UxDaj4BXAfeee+iAA2DBdc91wPqOx54EPJ/F7eLo5mK6Lz7PAf5qia/dzVuBe1CjQkZ5hGNpJnxj4FXAY6iD0/N+yL/4Bey1F8z0Wl/aSMXwOuCPqDner6hdE3sxmmOcm4HbgLuP4LWXlWk81rmJWtQNu7i7jdod4bVuGjSNW51rWDiSFr3/Vlpdvv8+wK1Dvt5yMX/6rgAeOa4J2c6EhzbndqA1++85N9N7z/dxwL91PLaO2krttI1adzsNeDlze//vYPkdctqVuZ/BPtQO4vGbmtAeDVzDAcAt8x7dHbi8x3ecCDyv47H5F+RrATdSsf6QWuH+L2q3yN9Ti90XAOfOPv/m2ec3NcdrUcOLFms9c/8Py+cCgxO+jtb2K2olewO1Mj/s38964F7UL2cTNXe7P91jfT3wOODps58fQh03vYwaobtUN1MbKefPfn7b7DT1G9fWnt6xBTZN62jXUlG1vQJYC/wx9Qu5lOH+Ro6gtg5bwI+BhwAP6vHcdzAXGdQiah3NRAawG3ORQc053wz8L/X/38sVLJe52HwTOkf7FLVu9PwuXzsX+D1qnWrIH/g3z4FDn8zcDtCNwM+Bhw73eo35AvBO6g/ruWOelp6mcfcGwHeA/YE9qHWoU4DPMnem0hCe82w4fRXMfGb2gR9Qox1OoRaRbecBDwR+c7j3WZLvUyv79xzDey9oWi+WfB61or4HcDY1pmzPpb3kGZ/teGBP4H7URsBp8x4/H7gb4wntAmodctmF1tUUzNHmezfwUppbT5rvZ9R62IOoMW1Po0aDzHci8Ep2/Pu9Gvg6O27lTqVp2Bi4FPjEAl/fleZWhG9nbngRwH7An1BHEnal+8Jglx7vvxPd98+tHBM2R7ua2nR/TMfjxwMvpvbsN2TLJnjZ3eBDL2G0lzAYwbSP1zSso92H7r+Qp1PrSg1atTMceSZ1mOpnwEtmv/BMakfwnsAfdnzTs6gt1FXUhskgRjDty9CEzdHmey/wAOoX37Rt1ADJ9gjcW6nFNtSGx2pqEXqPju/7AXV4aoYd199WjGnYvfEN6mylE6mdljtTc5em7Q98ntpPd36f5y5Hm6nVi7FM+zSEtoX6Ie7W4Gs+F3gjdQyzbQO1aLyOwXaVPJ7aYw/NHh0YVovBp71x0xDaKGym5ozzN8B3pw6Q99uCPZLanXIIc6M47jLA9021lRxaexzZoHtztlK7N3ajDlL/tMfreAvGLqZhP1rbHXQ9o3zTph7nClxKjbTotIXuZ5avorY2twA/mff4GdQJLG07YWSDmdDQ3k1tEHQ4+GC45ZYdH2cV3TcaXkbtse+lc251xOx7a7EmdNH5IWqxdQS1N759NZy11Cl0i/37uZraVdHvcgYLff89yeyWbK87jmJrG+pXvpYdz3kd2DQsOm+ixlv9BbX/7K3UKIa2Y6jxaIt1HEu7qs4r2X583CidTg0XGpVt1Li+Zk3YHO17wFeog9MfBg6lzud8AMtn73oL+BrwhHFPyLhMwxztEGoI9UWzHz+VOtSTucbX4EZ1EZnJNWFztEnwAWrf2ordGp2GOdqwrmFuLnM+Nfa+6SsJtd0wotedbBM2emNYOzN3OGYXar1u1xG91xv6P2UFctGpIZ0APIe6Jtx2VvKis0n/Tl32alDrqIGN0+YZbHcFpz6coy3atdThr70GfP5magdor/NDp8nBwAUr+aD6MDZTcdyP0W04TJurgPsZ2uK0r70xw/LZGTwRXEcrH6GunH0cNUDx7B7Pm6Gu59ErshZ1VtQ58z5v//euXZ7b7W/2WayUu9utwNBa1PG8d1HHRZ+0iO/dxtwAx5dRo3MPnf18P2of2gx1THb+MKbz2fFEFqgz6h+1iPefXC46F+UM4FvUBV5m6D3aYyt1VaIrqHFtm1lB53W6jtacv6O2Oo9e4Dm3Uldb/E9q98Z/j36ylgdDa8YN1OJzD2A9XLUR7rEP3HmhE1I2UbtFOi83302LOo90YneHuDGwNDdR5w6cDXwWLr8crjse3nYkrF3b43tup0aaXEYdmrpswPd64VIndtlxjjawHwBfhCuPgi1b4EtfgoMOgsd1Oxeh7Qbg1cC/UAM0v0Zdcn6queisPfRXUVe8Hcbl8OVTYeNvw+GH93hOi7pF4VHzHtsAXMhk30R2YC46awfsVUN+7xXAm+BJ6+ZF9nkq3o+y/YWNL6r/3HornHwytRtl0MXmdFohw4TaDpz9ZynfP/+aGtuY2y/XNkNd/rP9lG3A3sCfL+F9J98KW3T28hbgLxns6olfoa4YdBjwfmqHb+cNVrdRo2w/2NwkTg4Xnb09kcEHQu5HXa0bal2v84pCUKGdQveLOU+S9dTGzNI5RxuJFnUR551ZLrfIGc4m6kz9Qa9G/jjgHLc6p8+LqEs0HDDuCZn1f8D+hjZ9rqMuRDPsGfYj4TpaRov+t9Fpyh7sGNk2xnM5+oUZ2tCOpYb5dJqhTu/rlLoF4ww73l25qWn5B2p41UK3puzO0Ib2fmrgYjfdFmVnA382usn5tYWGL7W9nDrJZrFeDVxPDR7tdvvIzgGf86bKdTQt7A5qPN383T/HUmdBHUqNs1s1/xtcR8u4msH/Pq+lLva3nF1Anb/ZdiN15OOp1KVVfz7Qqxha4/Zl8NDeyNxFlifFydQtiq6k7uA82Lg5Q1uyi6i7Hp9HLWYeu4jv/WfgoCW89yXU4bBRuivb76d7LfBsah31Mup8iP5zZUNbstOoYUAfotZlvkrux3o6o79k10PoehlXnk3dxeYjbH978e7cGBiZ26kIj+r3xAn1Xur2Qvt1fsGNgawWtR6zkNMYfnzcuL2CLpH1ZGgjcyf6X8JqDbVr4ATmBk6+lslckPw18KqeXzW0sTqMuiLPQ5nbyfq+8U3Odm5icUOEHsH2twHfnutoy86XqfFxi7006RHUPq6zaGb+sYXakj603xM7OXpjul1I7V55KXU9j16hnkQddD+igfc8mhqd/ID5DxraaDyP+mHvcOXDEXob8DBq0dtpZ2pu1Cu066hTRZq4IcY6ahTxWmp/4q70emPX0ZbkGGpf1hbqNow3dnx9AzX3eHJD7/fx2dc7gd47atez8GJ3D5q768p9qYPz32Xu7jXdrbCzoJr0CuoX/0Vqz/n1zC0AtlLX5vgVtYuj39/zQdT9Eu7V53nPo/bG70Lvu5skxqK9GDgceBqDjr1z0Tm09ql27TvcbZ33cfsUvFU9v3t78793kPeF7eNtP5a6peOCt4900dmsnaiQ2j/X+R/PMHhknd87yPt2/toOpNaPhrkP1jAWf/tI52hjcSsV17hviT0SztGWj/cCnxv3REQ5RxuLq6m52Z79njiJnKMtH2dSt4RcrrYydxHotvVsf9vvxTG0qMupERuPp4ZCLzct4DPUEKd/7fjaT6iz7y9k0OHb8xlaYz5KDXxcyDXUQfPlvH72TWpUyYc7Hn888ALqj2Xxgy1dR2vMe6gdmWu6fG0ddWWhh1OjUrdR0V1AXc5q79A0RriONlqvpHtkUMcf703dIOPR1HkFX6DOghrkcgY/ow6GTy4PQY3cL6gzvN/Z8fix1Nyt32GntdTZ4X/a/KQFuegcuduAS9n+SpGLcQt1lZ6DOx5/G7VB0TnYcBvwFHrfemjkHCY0XdZSi+LdOh5vAT9kaZdQXRJDy2gBD2SYXQBTwo2BjBlqa3KpbmbuEqaTz9BG4u4NvMZdqZtotH2eGqa93Ax2CSwXnctOizoE1LlDoP2rSIw3W4wDqRNqfr317DraZGhRw62vH/eELMKt1OWrZsB1tEkxw2RFBnVz24Wvv+EcTU1zjqbxMTQ16KyeXzE0NehTPb/iOpqa5jqaxsfQptoG4M3jngjA0KbcLtT9REflOAa9YqXraFqCS6jLi95l/oOuo02HDwIf6/OczdTe+rZvU5fV+hvq3Ib21uFRwI+XMC0H0hFZT87RJs5Gaqax0B2TW9TJL3vNfv4V6uz4jzN3XZBdqHMWdqfh2zA6R5sOJ1EjJT69wHNmmIusbQ0V1W7MXctsT1L3+jS0ifMa6qqNTVwadLHat1FcvH6LTqkRztEUYWiKMDRFGJoiDE0RhqaI/wfwRgcWBB1eQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_training_images, _ = next(train_data_gen)\n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43be810e",
   "metadata": {},
   "source": [
    "Recreate the `train_image_generator` using `ImageDataGenerator`. \n",
    "\n",
    "Since there are a small number of training examples there is a risk of overfitting. One way to fix this problem is by creating more training data from existing training examples by using random transformations.\n",
    "\n",
    "Add 4-6 random transformations as arguments to `ImageDataGenerator`. Make sure to rescale the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "adc4629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(train_dir,rescale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8034eda",
   "metadata": {},
   "source": [
    "You don't have to do anything for the next cell. `train_data_gen` is created just like before but with the new `train_image_generator`. Then, a single image is plotted five different times using different variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "85d93a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                           directory=train_dir,\n",
    "                                                           target_size=(\n",
    "                                                               IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary')\n",
    "\n",
    "augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n",
    "\n",
    "plotImages(augmented_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b6398",
   "metadata": {},
   "source": [
    "In the cell below, create a model for the neural network that outputs class probabilities. It should use the Keras Sequential model. It will probably involve a stack of Conv2D and MaxPooling2D layers and then a fully connected layer on top that is activated by a ReLU activation function.\n",
    "\n",
    "Compile the model passing the arguments to set the optimizer and loss. Also pass in `metrics=['accuracy']` to view training and validation accuracy for each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "561fec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 15, 15, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 6, 6, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,050\n",
      "Trainable params: 122,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation=\"relu\",input_shape=(32,32,3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fd4be8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2b068",
   "metadata": {},
   "source": [
    "Use the `fit` method on your `model` to train the network. Make sure to pass in arguments for `x`, `steps_per_epoch`, `epochs`, `validation_data`, and `validation_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ffeb4913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/pc/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/pc/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4442/3736794279.py\", line 1, in <cell line: 1>\n      history = model.fit(val_data_gen,epochs=epochs,validation_data=test_data_gen)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/losses.py\", line 267, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/losses.py\", line 2052, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/backend.py\", line 5626, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [9248,2] and labels shape [256]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_3025]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_gen\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/pc/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/pc/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/pc/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/pc/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4442/3736794279.py\", line 1, in <cell line: 1>\n      history = model.fit(val_data_gen,epochs=epochs,validation_data=test_data_gen)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/losses.py\", line 267, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/losses.py\", line 2052, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/pc/.local/lib/python3.8/site-packages/keras/backend.py\", line 5626, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [9248,2] and labels shape [256]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_3025]"
     ]
    }
   ],
   "source": [
    "history = model.fit(val_data_gen,epochs=epochs,validation_data=test_data_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3ab36",
   "metadata": {},
   "source": [
    "Run the next cell to visualize the accuracy and loss of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03939078",
   "metadata": {},
   "source": [
    "Now it is time to use your model to predict whether a brand new image is a cat or a dog.\n",
    "\n",
    "In this final cell, get the probability that each test image (from `test_data_gen`) is a dog or a cat. `probabilities` should be a list of integers. \n",
    "\n",
    "Call the `plotImages` function and pass in the test images and the probabilities corresponding to each test image.\n",
    "\n",
    "After your run the cell, you should see all 50 test images with a label showing the percentage sure that the image is a cat or a dog. The accuracy will correspond to the accuracy shown in the graph above (after running the previous cell). More training images could lead to a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f671de0",
   "metadata": {},
   "source": [
    "Run this final cell to see if you passed the challenge or if you need to keep trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
    "           1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
    "           1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
    "           1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
    "           0, 0, 0, 0, 0, 0]\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for probability, answer in zip(probabilities, answers):\n",
    "    if round(probability) == answer:\n",
    "        correct += 1\n",
    "\n",
    "percentage_identified = (correct / len(answers))\n",
    "\n",
    "passed_challenge = percentage_identified > 0.63\n",
    "\n",
    "print(\n",
    "    f\"Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs.\")\n",
    "\n",
    "if passed_challenge:\n",
    "    print(\"You passed the challenge!\")\n",
    "else:\n",
    "    print(\"You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff152a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe083a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
